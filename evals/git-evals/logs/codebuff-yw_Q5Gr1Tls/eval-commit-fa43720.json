{"sha":"fa437205fa35b3bc6833e59793b49cc3c8e613b8","spec":"Add support for reasoning options configuration in agent definitions.\n\n**Agent Definition Interface Changes:**\n- Add an optional `reasoningOptions` property to the `AgentDefinition` interface\n- The `reasoningOptions` should have the following structure:\n  - Optional `enabled` boolean field\n  - Optional `exclude` boolean field  \n  - Must include either a `max_tokens` number field OR an `effort` field with values 'high', 'medium', or 'low'\n- Include documentation referencing https://openrouter.ai/docs/use-cases/reasoning-tokens\n\n**Type System Updates:**\n- Update both the local agent types file and the common template types to include the new reasoning options\n- Add reasoning options validation to the dynamic agent definition schema\n- Ensure the `AgentTemplate` type includes a `reasoningOptions` field that uses the OpenRouter provider options type\n- Change the `ModelName` import in the base factory to be a type-only import\n\n**Agent Configuration:**\n- Update the base-lite agent to include reasoning options with enabled=true, exclude=false, and effort='high'\n\n**Backend Integration:**\n- Modify the agent stream generation to pass reasoning options from the agent template to the OpenRouter provider options\n- Remove Google-specific provider options configuration that was hardcoded\n- Ensure the reasoning options from the agent template are properly passed through to the OpenRouter provider when making API calls\n- Always initialize provider options and set the OpenRouter reasoning configuration from the template\n\n**Schema Validation:**\n- Add proper validation for the reasoning options in the dynamic agent definition schema\n- Ensure the reasoning options follow the constraint that either max_tokens or effort must be provided, but not both\n\nThe reasoning options should integrate with OpenRouter's reasoning tokens feature and be configurable per agent definition while maintaining backwards compatibility for existing agents without reasoning options specified.","fileStates":[{"path":".agents/base-lite.ts","preContent":"import { publisher } from './constants'\nimport { base } from './factory/base'\n\nimport type { SecretAgentDefinition } from './types/secret-agent-definition'\n\nconst definition: SecretAgentDefinition = {\n  id: 'base-lite',\n  publisher,\n  ...base('openai/gpt-5'),\n}\n\nexport default definition\n","postContent":"import { publisher } from './constants'\nimport { base } from './factory/base'\n\nimport type { SecretAgentDefinition } from './types/secret-agent-definition'\n\nconst definition: SecretAgentDefinition = {\n  id: 'base-lite',\n  publisher,\n  ...base('openai/gpt-5'),\n  reasoningOptions: {\n    enabled: true,\n    exclude: false,\n    effort: 'high',\n  },\n}\n\nexport default definition\n"},{"path":".agents/factory/base.ts","preContent":"import { AGENT_PERSONAS } from '@codebuff/common/constants/agents'\n\nimport {\n  baseAgentAgentStepPrompt,\n  baseAgentSystemPrompt,\n  baseAgentUserInputPrompt,\n} from '../prompts'\nimport { AgentTemplateTypes } from '../types/secret-agent-definition'\n\nimport type { SecretAgentDefinition } from '../types/secret-agent-definition'\nimport { ModelName } from 'types/agent-definition'\n\nexport const base = (\n  model: ModelName,\n  allAvailableAgents?: string[],\n): Omit<SecretAgentDefinition, 'id'> => ({\n  model,\n  displayName: AGENT_PERSONAS.base.displayName,\n  spawnerPrompt: AGENT_PERSONAS.base.purpose,\n  inputSchema: {\n    prompt: {\n      type: 'string',\n      description: 'A coding task to complete',\n    },\n  },\n  outputMode: 'last_message',\n  includeMessageHistory: false,\n  toolNames: [\n    'create_plan',\n    'run_terminal_command',\n    'str_replace',\n    'write_file',\n    'spawn_agents',\n    'add_subgoal',\n    'browser_logs',\n    'code_search',\n    'end_turn',\n    'read_files',\n    'think_deeply',\n    'update_subgoal',\n  ],\n  spawnableAgents: allAvailableAgents\n    ? (allAvailableAgents as any[])\n    : [\n        AgentTemplateTypes.file_explorer,\n        AgentTemplateTypes.file_picker,\n        AgentTemplateTypes.researcher,\n        AgentTemplateTypes.thinker,\n        AgentTemplateTypes.reviewer,\n      ],\n\n  systemPrompt: baseAgentSystemPrompt(model),\n  instructionsPrompt: baseAgentUserInputPrompt(model),\n  stepPrompt: baseAgentAgentStepPrompt(model),\n})\n","postContent":"import { AGENT_PERSONAS } from '@codebuff/common/constants/agents'\n\nimport {\n  baseAgentAgentStepPrompt,\n  baseAgentSystemPrompt,\n  baseAgentUserInputPrompt,\n} from '../prompts'\nimport { AgentTemplateTypes } from '../types/secret-agent-definition'\n\nimport type { SecretAgentDefinition } from '../types/secret-agent-definition'\nimport type { ModelName } from 'types/agent-definition'\n\nexport const base = (\n  model: ModelName,\n  allAvailableAgents?: string[],\n): Omit<SecretAgentDefinition, 'id'> => ({\n  model,\n  displayName: AGENT_PERSONAS.base.displayName,\n  spawnerPrompt: AGENT_PERSONAS.base.purpose,\n  inputSchema: {\n    prompt: {\n      type: 'string',\n      description: 'A coding task to complete',\n    },\n  },\n  outputMode: 'last_message',\n  includeMessageHistory: false,\n  toolNames: [\n    'create_plan',\n    'run_terminal_command',\n    'str_replace',\n    'write_file',\n    'spawn_agents',\n    'add_subgoal',\n    'browser_logs',\n    'code_search',\n    'end_turn',\n    'read_files',\n    'think_deeply',\n    'update_subgoal',\n  ],\n  spawnableAgents: allAvailableAgents\n    ? (allAvailableAgents as any[])\n    : [\n        AgentTemplateTypes.file_explorer,\n        AgentTemplateTypes.file_picker,\n        AgentTemplateTypes.researcher,\n        AgentTemplateTypes.thinker,\n        AgentTemplateTypes.reviewer,\n      ],\n\n  systemPrompt: baseAgentSystemPrompt(model),\n  instructionsPrompt: baseAgentUserInputPrompt(model),\n  stepPrompt: baseAgentAgentStepPrompt(model),\n})\n"},{"path":".agents/types/agent-definition.ts","preContent":"/**\n * Codebuff Agent Type Definitions\n *\n * This file provides TypeScript type definitions for creating custom Codebuff agents.\n * Import these types in your agent files to get full type safety and IntelliSense.\n *\n * Usage in .agents/your-agent.ts:\n *   import { AgentDefinition, ToolName, ModelName } from './types/agent-definition'\n *\n *   const definition: AgentDefinition = {\n *     // ... your agent configuration with full type safety ...\n *   }\n *\n *   export default definition\n */\n\n// ============================================================================\n// Agent Definition and Utility Types\n// ============================================================================\n\nexport interface AgentDefinition {\n  /** Unique identifier for this agent. Must contain only lowercase letters, numbers, and hyphens, e.g. 'code-reviewer' */\n  id: string\n\n  /** Version string (if not provided, will default to '0.0.1' and be bumped on each publish) */\n  version?: string\n\n  /** Publisher ID for the agent. Must be provided if you want to publish the agent. */\n  publisher?: string\n\n  /** Human-readable name for the agent */\n  displayName: string\n\n  /** AI model to use for this agent. Can be any model in OpenRouter: https://openrouter.ai/models */\n  model: ModelName\n\n  // ============================================================================\n  // Tools and Subagents\n  // ============================================================================\n\n  /** Tools this agent can use. */\n  toolNames?: ToolName[]\n\n  /** Other agents this agent can spawn, like 'codebuff/file-picker@0.0.1'.\n   *\n   * Use the fully qualified agent id from the agent store, including publisher and version: 'codebuff/file-picker@0.0.1'\n   * (publisher and version are required!)\n   *\n   * Or, use the agent id from a local agent file in your .agents directory: 'file-picker'.\n   */\n  spawnableAgents?: string[]\n\n  // ============================================================================\n  // Input and Output\n  // ============================================================================\n\n  /** The input schema required to spawn the agent. Provide a prompt string and/or a params object or none.\n   * 80% of the time you want just a prompt string with a description:\n   * inputSchema: {\n   *   prompt: { type: 'string', description: 'A description of what info would be helpful to the agent' }\n   * }\n   */\n  inputSchema?: {\n    prompt?: { type: 'string'; description?: string }\n    params?: JsonObjectSchema\n  }\n\n  /** Whether to include conversation history from the parent agent in context.\n   *\n   * Defaults to false.\n   * Use this if the agent needs to know all the previous messages in the conversation.\n   */\n  includeMessageHistory?: boolean\n\n  /** How the agent should output a response to its parent (defaults to 'last_message')\n   *\n   * last_message: The last message from the agent, typcically after using tools.\n   *\n   * all_messages: All messages from the agent, including tool calls and results.\n   *\n   * json: Make the agent output a JSON object. Can be used with outputSchema or without if you want freeform json output.\n   */\n  outputMode?: 'last_message' | 'all_messages' | 'structured_output'\n\n  /** JSON schema for structured output (when outputMode is 'structured_output') */\n  outputSchema?: JsonObjectSchema\n\n  // ============================================================================\n  // Prompts\n  // ============================================================================\n\n  /** Prompt for when and why to spawn this agent. Include the main purpose and use cases.\n   *\n   * This field is key if the agent is intended to be spawned by other agents. */\n  spawnerPrompt?: string\n\n  /** Background information for the agent. Fairly optional. Prefer using instructionsPrompt for agent instructions. */\n  systemPrompt?: string\n\n  /** Instructions for the agent.\n   *\n   * IMPORTANT: Updating this prompt is the best way to shape the agent's behavior.\n   * This prompt is inserted after each user input. */\n  instructionsPrompt?: string\n\n  /** Prompt inserted at each agent step.\n   *\n   * Powerful for changing the agent's behavior, but usually not necessary for smart models.\n   * Prefer instructionsPrompt for most instructions. */\n  stepPrompt?: string\n\n  // ============================================================================\n  // Handle Steps\n  // ============================================================================\n\n  /** Programmatically step the agent forward and run tools.\n   *\n   * You can either yield:\n   * - A tool call object with toolName and input properties.\n   * - 'STEP' to run agent's model and generate one assistant message.\n   * - 'STEP_ALL' to run the agent's model until it uses the end_turn tool or stops includes no tool calls in a message.\n   *\n   * Or use 'return' to end the turn.\n   *\n   * Example 1:\n   * function* handleSteps({ agentStep, prompt, params}) {\n   *   const { toolResult } = yield {\n   *     toolName: 'read_files',\n   *     input: { paths: ['file1.txt', 'file2.txt'] }\n   *   }\n   *   yield 'STEP_ALL'\n   * }\n   *\n   * Example 2:\n   * handleSteps: function* ({ agentState, prompt, params }) {\n   *   while (true) {\n   *     yield {\n   *       toolName: 'spawn_agents',\n   *       input: {\n   *         agents: [\n   *         {\n   *           agent_type: 'thinker',\n   *           prompt: 'Think deeply about the user request',\n   *         },\n   *       ],\n   *     },\n   *   }\n   *   yield 'STEP'\n   * }\n   * }\n   */\n  handleSteps?: (\n    context: AgentStepContext,\n  ) => Generator<\n    ToolCall | 'STEP' | 'STEP_ALL',\n    void,\n    { agentState: AgentState; toolResult: string | undefined }\n  >\n}\n\n// ============================================================================\n// Supporting Types\n// ============================================================================\n\nexport interface AgentState {\n  agentId: string\n  parentId: string\n  messageHistory: Message[]\n}\n\n/**\n * Message in conversation history\n */\nexport interface Message {\n  role: 'user' | 'assistant'\n  content: string\n}\n\n/**\n * Context provided to handleSteps generator function\n */\nexport interface AgentStepContext {\n  agentState: AgentState\n  prompt?: string\n  params?: Record<string, any>\n}\n\n/**\n * Tool call object for handleSteps generator\n */\nexport type ToolCall<T extends ToolName = ToolName> = {\n  [K in T]: {\n    toolName: K\n    input: Tools.GetToolParams<K>\n  }\n}[T]\n\n/**\n * JSON Schema definition (for prompt schema or output schema)\n */\nexport type JsonSchema = {\n  type?:\n    | 'object'\n    | 'array'\n    | 'string'\n    | 'number'\n    | 'boolean'\n    | 'null'\n    | 'integer'\n  description?: string\n  properties?: Record<string, JsonSchema | boolean>\n  required?: string[]\n  enum?: Array<string | number | boolean | null>\n  [k: string]: unknown\n}\nexport type JsonObjectSchema = JsonSchema & { type: 'object' }\n\n// ============================================================================\n// Available Tools\n// ============================================================================\n\n/**\n * File operation tools\n */\nexport type FileTools =\n  | 'read_files'\n  | 'write_file'\n  | 'str_replace'\n  | 'find_files'\n\n/**\n * Code analysis tools\n */\nexport type CodeAnalysisTools = 'code_search' | 'find_files'\n\n/**\n * Terminal and system tools\n */\nexport type TerminalTools = 'run_terminal_command' | 'run_file_change_hooks'\n\n/**\n * Web and browser tools\n */\nexport type WebTools = 'web_search' | 'read_docs'\n\n/**\n * Agent management tools\n */\nexport type AgentTools = 'spawn_agents' | 'set_messages' | 'add_message'\n\n/**\n * Planning and organization tools\n */\nexport type PlanningTools = 'think_deeply'\n\n/**\n * Output and control tools\n */\nexport type OutputTools = 'set_output' | 'end_turn'\n\n/**\n * Common tool combinations for convenience\n */\nexport type FileEditingTools = FileTools | 'end_turn'\nexport type ResearchTools = WebTools | 'write_file' | 'end_turn'\nexport type CodeAnalysisToolSet = FileTools | CodeAnalysisTools | 'end_turn'\n\n// ============================================================================\n// Available Models (see: https://openrouter.ai/models)\n// ============================================================================\n\n/**\n * AI models available for agents. Pick from our selection of recommended models or choose any model in OpenRouter.\n *\n * See available models at https://openrouter.ai/models\n */\nexport type ModelName =\n  // Recommended Models\n\n  // OpenAI\n  | 'openai/gpt-5'\n  | 'openai/gpt-5-mini'\n  | 'openai/gpt-5-nano'\n\n  // Anthropic\n  | 'anthropic/claude-4-sonnet-20250522'\n  | 'anthropic/claude-opus-4.1'\n\n  // Gemini\n  | 'google/gemini-2.5-pro'\n  | 'google/gemini-2.5-flash'\n  | 'google/gemini-2.5-flash-lite'\n\n  // X-AI\n  | 'x-ai/grok-4-07-09'\n\n  // Qwen\n  | 'qwen/qwen3-coder'\n  | 'qwen/qwen3-coder:nitro'\n  | 'qwen/qwen3-235b-a22b-2507'\n  | 'qwen/qwen3-235b-a22b-2507:nitro'\n  | 'qwen/qwen3-235b-a22b-thinking-2507'\n  | 'qwen/qwen3-235b-a22b-thinking-2507:nitro'\n  | 'qwen/qwen3-30b-a3b'\n  | 'qwen/qwen3-30b-a3b:nitro'\n\n  // DeepSeek\n  | 'deepseek/deepseek-chat-v3-0324'\n  | 'deepseek/deepseek-chat-v3-0324:nitro'\n  | 'deepseek/deepseek-r1-0528'\n  | 'deepseek/deepseek-r1-0528:nitro'\n\n  // Other open source models\n  | 'moonshotai/kimi-k2'\n  | 'moonshotai/kimi-k2:nitro'\n  | 'z-ai/glm-4.5'\n  | 'z-ai/glm-4.5:nitro'\n  | (string & {})\n\nimport type * as Tools from './tools'\nexport type { Tools }\ntype ToolName = Tools.ToolName\n","postContent":"/**\n * Codebuff Agent Type Definitions\n *\n * This file provides TypeScript type definitions for creating custom Codebuff agents.\n * Import these types in your agent files to get full type safety and IntelliSense.\n *\n * Usage in .agents/your-agent.ts:\n *   import { AgentDefinition, ToolName, ModelName } from './types/agent-definition'\n *\n *   const definition: AgentDefinition = {\n *     // ... your agent configuration with full type safety ...\n *   }\n *\n *   export default definition\n */\n\n// ============================================================================\n// Agent Definition and Utility Types\n// ============================================================================\n\nexport interface AgentDefinition {\n  /** Unique identifier for this agent. Must contain only lowercase letters, numbers, and hyphens, e.g. 'code-reviewer' */\n  id: string\n\n  /** Version string (if not provided, will default to '0.0.1' and be bumped on each publish) */\n  version?: string\n\n  /** Publisher ID for the agent. Must be provided if you want to publish the agent. */\n  publisher?: string\n\n  /** Human-readable name for the agent */\n  displayName: string\n\n  /** AI model to use for this agent. Can be any model in OpenRouter: https://openrouter.ai/models */\n  model: ModelName\n\n  /**\n   * https://openrouter.ai/docs/use-cases/reasoning-tokens\n   * One of `max_tokens` or `effort` is required.\n   * If `exclude` is true, reasoning will be removed from the response. Default is false.\n   */\n  reasoningOptions?: {\n    enabled?: boolean\n    exclude?: boolean\n  } & (\n    | {\n        max_tokens: number\n      }\n    | {\n        effort: 'high' | 'medium' | 'low'\n      }\n  )\n\n  // ============================================================================\n  // Tools and Subagents\n  // ============================================================================\n\n  /** Tools this agent can use. */\n  toolNames?: ToolName[]\n\n  /** Other agents this agent can spawn, like 'codebuff/file-picker@0.0.1'.\n   *\n   * Use the fully qualified agent id from the agent store, including publisher and version: 'codebuff/file-picker@0.0.1'\n   * (publisher and version are required!)\n   *\n   * Or, use the agent id from a local agent file in your .agents directory: 'file-picker'.\n   */\n  spawnableAgents?: string[]\n\n  // ============================================================================\n  // Input and Output\n  // ============================================================================\n\n  /** The input schema required to spawn the agent. Provide a prompt string and/or a params object or none.\n   * 80% of the time you want just a prompt string with a description:\n   * inputSchema: {\n   *   prompt: { type: 'string', description: 'A description of what info would be helpful to the agent' }\n   * }\n   */\n  inputSchema?: {\n    prompt?: { type: 'string'; description?: string }\n    params?: JsonObjectSchema\n  }\n\n  /** Whether to include conversation history from the parent agent in context.\n   *\n   * Defaults to false.\n   * Use this if the agent needs to know all the previous messages in the conversation.\n   */\n  includeMessageHistory?: boolean\n\n  /** How the agent should output a response to its parent (defaults to 'last_message')\n   *\n   * last_message: The last message from the agent, typcically after using tools.\n   *\n   * all_messages: All messages from the agent, including tool calls and results.\n   *\n   * json: Make the agent output a JSON object. Can be used with outputSchema or without if you want freeform json output.\n   */\n  outputMode?: 'last_message' | 'all_messages' | 'structured_output'\n\n  /** JSON schema for structured output (when outputMode is 'structured_output') */\n  outputSchema?: JsonObjectSchema\n\n  // ============================================================================\n  // Prompts\n  // ============================================================================\n\n  /** Prompt for when and why to spawn this agent. Include the main purpose and use cases.\n   *\n   * This field is key if the agent is intended to be spawned by other agents. */\n  spawnerPrompt?: string\n\n  /** Background information for the agent. Fairly optional. Prefer using instructionsPrompt for agent instructions. */\n  systemPrompt?: string\n\n  /** Instructions for the agent.\n   *\n   * IMPORTANT: Updating this prompt is the best way to shape the agent's behavior.\n   * This prompt is inserted after each user input. */\n  instructionsPrompt?: string\n\n  /** Prompt inserted at each agent step.\n   *\n   * Powerful for changing the agent's behavior, but usually not necessary for smart models.\n   * Prefer instructionsPrompt for most instructions. */\n  stepPrompt?: string\n\n  // ============================================================================\n  // Handle Steps\n  // ============================================================================\n\n  /** Programmatically step the agent forward and run tools.\n   *\n   * You can either yield:\n   * - A tool call object with toolName and input properties.\n   * - 'STEP' to run agent's model and generate one assistant message.\n   * - 'STEP_ALL' to run the agent's model until it uses the end_turn tool or stops includes no tool calls in a message.\n   *\n   * Or use 'return' to end the turn.\n   *\n   * Example 1:\n   * function* handleSteps({ agentStep, prompt, params}) {\n   *   const { toolResult } = yield {\n   *     toolName: 'read_files',\n   *     input: { paths: ['file1.txt', 'file2.txt'] }\n   *   }\n   *   yield 'STEP_ALL'\n   * }\n   *\n   * Example 2:\n   * handleSteps: function* ({ agentState, prompt, params }) {\n   *   while (true) {\n   *     yield {\n   *       toolName: 'spawn_agents',\n   *       input: {\n   *         agents: [\n   *         {\n   *           agent_type: 'thinker',\n   *           prompt: 'Think deeply about the user request',\n   *         },\n   *       ],\n   *     },\n   *   }\n   *   yield 'STEP'\n   * }\n   * }\n   */\n  handleSteps?: (\n    context: AgentStepContext,\n  ) => Generator<\n    ToolCall | 'STEP' | 'STEP_ALL',\n    void,\n    { agentState: AgentState; toolResult: string | undefined }\n  >\n}\n\n// ============================================================================\n// Supporting Types\n// ============================================================================\n\nexport interface AgentState {\n  agentId: string\n  parentId: string\n  messageHistory: Message[]\n}\n\n/**\n * Message in conversation history\n */\nexport interface Message {\n  role: 'user' | 'assistant'\n  content: string\n}\n\n/**\n * Context provided to handleSteps generator function\n */\nexport interface AgentStepContext {\n  agentState: AgentState\n  prompt?: string\n  params?: Record<string, any>\n}\n\n/**\n * Tool call object for handleSteps generator\n */\nexport type ToolCall<T extends ToolName = ToolName> = {\n  [K in T]: {\n    toolName: K\n    input: Tools.GetToolParams<K>\n  }\n}[T]\n\n/**\n * JSON Schema definition (for prompt schema or output schema)\n */\nexport type JsonSchema = {\n  type?:\n    | 'object'\n    | 'array'\n    | 'string'\n    | 'number'\n    | 'boolean'\n    | 'null'\n    | 'integer'\n  description?: string\n  properties?: Record<string, JsonSchema | boolean>\n  required?: string[]\n  enum?: Array<string | number | boolean | null>\n  [k: string]: unknown\n}\nexport type JsonObjectSchema = JsonSchema & { type: 'object' }\n\n// ============================================================================\n// Available Tools\n// ============================================================================\n\n/**\n * File operation tools\n */\nexport type FileTools =\n  | 'read_files'\n  | 'write_file'\n  | 'str_replace'\n  | 'find_files'\n\n/**\n * Code analysis tools\n */\nexport type CodeAnalysisTools = 'code_search' | 'find_files'\n\n/**\n * Terminal and system tools\n */\nexport type TerminalTools = 'run_terminal_command' | 'run_file_change_hooks'\n\n/**\n * Web and browser tools\n */\nexport type WebTools = 'web_search' | 'read_docs'\n\n/**\n * Agent management tools\n */\nexport type AgentTools = 'spawn_agents' | 'set_messages' | 'add_message'\n\n/**\n * Planning and organization tools\n */\nexport type PlanningTools = 'think_deeply'\n\n/**\n * Output and control tools\n */\nexport type OutputTools = 'set_output' | 'end_turn'\n\n/**\n * Common tool combinations for convenience\n */\nexport type FileEditingTools = FileTools | 'end_turn'\nexport type ResearchTools = WebTools | 'write_file' | 'end_turn'\nexport type CodeAnalysisToolSet = FileTools | CodeAnalysisTools | 'end_turn'\n\n// ============================================================================\n// Available Models (see: https://openrouter.ai/models)\n// ============================================================================\n\n/**\n * AI models available for agents. Pick from our selection of recommended models or choose any model in OpenRouter.\n *\n * See available models at https://openrouter.ai/models\n */\nexport type ModelName =\n  // Recommended Models\n\n  // OpenAI\n  | 'openai/gpt-5'\n  | 'openai/gpt-5-mini'\n  | 'openai/gpt-5-nano'\n\n  // Anthropic\n  | 'anthropic/claude-4-sonnet-20250522'\n  | 'anthropic/claude-opus-4.1'\n\n  // Gemini\n  | 'google/gemini-2.5-pro'\n  | 'google/gemini-2.5-flash'\n  | 'google/gemini-2.5-flash-lite'\n\n  // X-AI\n  | 'x-ai/grok-4-07-09'\n\n  // Qwen\n  | 'qwen/qwen3-coder'\n  | 'qwen/qwen3-coder:nitro'\n  | 'qwen/qwen3-235b-a22b-2507'\n  | 'qwen/qwen3-235b-a22b-2507:nitro'\n  | 'qwen/qwen3-235b-a22b-thinking-2507'\n  | 'qwen/qwen3-235b-a22b-thinking-2507:nitro'\n  | 'qwen/qwen3-30b-a3b'\n  | 'qwen/qwen3-30b-a3b:nitro'\n\n  // DeepSeek\n  | 'deepseek/deepseek-chat-v3-0324'\n  | 'deepseek/deepseek-chat-v3-0324:nitro'\n  | 'deepseek/deepseek-r1-0528'\n  | 'deepseek/deepseek-r1-0528:nitro'\n\n  // Other open source models\n  | 'moonshotai/kimi-k2'\n  | 'moonshotai/kimi-k2:nitro'\n  | 'z-ai/glm-4.5'\n  | 'z-ai/glm-4.5:nitro'\n  | (string & {})\n\nimport type * as Tools from './tools'\nexport type { Tools }\ntype ToolName = Tools.ToolName\n"},{"path":"backend/src/llm-apis/vercel-ai-sdk/ai-sdk.ts","preContent":"import { google } from '@ai-sdk/google'\nimport { openai } from '@ai-sdk/openai'\nimport {\n  finetunedVertexModels,\n  geminiModels,\n  openaiModels,\n} from '@codebuff/common/old-constants'\nimport {\n  endToolTag,\n  startToolTag,\n  toolNameParam,\n} from '@codebuff/common/tools/constants'\nimport { buildArray } from '@codebuff/common/util/array'\nimport { errorToObject } from '@codebuff/common/util/object'\nimport { withTimeout } from '@codebuff/common/util/promise'\nimport { generateCompactId } from '@codebuff/common/util/string'\nimport { APICallError, generateObject, generateText, streamText } from 'ai'\n\nimport { checkLiveUserInput, getLiveUserInputIds } from '../../live-user-inputs'\nimport { logger } from '../../util/logger'\nimport { saveMessage } from '../message-cost-tracker'\nimport { openRouterLanguageModel } from '../openrouter'\nimport { vertexFinetuned } from './vertex-finetuned'\n\nimport type { System } from '../claude'\nimport type { GoogleGenerativeAIProviderOptions } from '@ai-sdk/google'\nimport type {\n  GeminiModel,\n  Model,\n  OpenAIModel,\n} from '@codebuff/common/old-constants'\nimport type { CodebuffMessage, Message } from '@codebuff/common/types/message'\nimport type { OpenRouterUsageAccounting } from '@openrouter/ai-sdk-provider'\nimport type { AssistantModelMessage, UserModelMessage, LanguageModel } from 'ai'\nimport type { z } from 'zod/v4'\n\n// TODO: We'll want to add all our models here!\nconst modelToAiSDKModel = (model: Model): LanguageModel => {\n  if (\n    Object.values(finetunedVertexModels as Record<string, string>).includes(\n      model,\n    )\n  ) {\n    return vertexFinetuned(model)\n  }\n  if (Object.values(geminiModels).includes(model as GeminiModel)) {\n    return google.languageModel(model)\n  }\n  if (model === openaiModels.o3pro || model === openaiModels.o3) {\n    return openai.responses(model)\n  }\n  if (Object.values(openaiModels).includes(model as OpenAIModel)) {\n    return openai.languageModel(model)\n  }\n  // All other models go through OpenRouter\n  return openRouterLanguageModel(model)\n}\n\n// TODO: Add retries & fallbacks: likely by allowing this to instead of \"model\"\n// also take an array of form [{model: Model, retries: number}, {model: Model, retries: number}...]\n// eg: [{model: \"gemini-2.0-flash-001\"}, {model: \"vertex/gemini-2.0-flash-001\"}, {model: \"claude-3-5-haiku\", retries: 3}]\nexport const promptAiSdkStream = async function* (\n  options: {\n    messages: CodebuffMessage[]\n    clientSessionId: string\n    fingerprintId: string\n    model: Model\n    userId: string | undefined\n    chargeUser?: boolean\n    thinkingBudget?: number\n    userInputId: string\n    maxRetries?: number\n  } & Omit<Parameters<typeof streamText>[0], 'model'>,\n) {\n  if (\n    !checkLiveUserInput(\n      options.userId,\n      options.userInputId,\n      options.clientSessionId,\n    )\n  ) {\n    logger.info(\n      {\n        userId: options.userId,\n        userInputId: options.userInputId,\n        liveUserInputId: getLiveUserInputIds(options.userId),\n      },\n      'Skipping stream due to canceled user input',\n    )\n    yield ''\n    return\n  }\n  const startTime = Date.now()\n\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const response = streamText({\n    ...options,\n    model: aiSDKModel,\n    maxRetries: options.maxRetries,\n    providerOptions: {\n      google: {\n        thinkingConfig: {\n          includeThoughts: false,\n          thinkingBudget: options.thinkingBudget ?? 128,\n        },\n      } satisfies GoogleGenerativeAIProviderOptions,\n    },\n  })\n\n  let content = ''\n  let reasoning = false\n\n  for await (const chunk of response.fullStream) {\n    if (chunk.type === 'error') {\n      logger.error(\n        {\n          chunk: { ...chunk, error: undefined },\n          error: errorToObject(chunk.error),\n          model: options.model,\n        },\n        'Error from AI SDK',\n      )\n\n      const errorBody = APICallError.isInstance(chunk.error)\n        ? chunk.error.responseBody\n        : undefined\n      const mainErrorMessage =\n        chunk.error instanceof Error\n          ? chunk.error.message\n          : typeof chunk.error === 'string'\n            ? chunk.error\n            : JSON.stringify(chunk.error)\n      const errorMessage = `Error from AI SDK (model ${options.model}): ${buildArray([mainErrorMessage, errorBody]).join('\\n')}`\n      throw new Error(errorMessage, {\n        cause: chunk.error,\n      })\n    }\n    if (chunk.type === 'reasoning-delta') {\n      if (!reasoning) {\n        reasoning = true\n        yield `${startToolTag}{\n  ${JSON.stringify(toolNameParam)}: \"think_deeply\",\n  \"thought\": \"`\n      }\n      yield JSON.stringify(chunk.text).slice(1, -1)\n    }\n    if (chunk.type === 'text-delta') {\n      if (reasoning) {\n        reasoning = false\n        yield `\"\\n}${endToolTag}\\n\\n`\n      }\n      content += chunk.text\n      yield chunk.text\n    }\n  }\n\n  const messageId = (await response.response).id\n  const providerMetadata = (await response.providerMetadata) ?? {}\n  const usage = await response.usage\n  let inputTokens = usage.inputTokens || 0\n  const outputTokens = usage.outputTokens || 0\n  let cacheReadInputTokens: number = 0\n  let cacheCreationInputTokens: number = 0\n  let costOverrideDollars: number | undefined\n  if (providerMetadata.anthropic) {\n    cacheReadInputTokens =\n      typeof providerMetadata.anthropic.cacheReadInputTokens === 'number'\n        ? providerMetadata.anthropic.cacheReadInputTokens\n        : 0\n    cacheCreationInputTokens =\n      typeof providerMetadata.anthropic.cacheCreationInputTokens === 'number'\n        ? providerMetadata.anthropic.cacheCreationInputTokens\n        : 0\n  }\n  if (providerMetadata.openrouter) {\n    if (providerMetadata.openrouter.usage) {\n      const openrouterUsage = providerMetadata.openrouter\n        .usage as OpenRouterUsageAccounting\n      cacheReadInputTokens =\n        openrouterUsage.promptTokensDetails?.cachedTokens ?? 0\n      inputTokens = openrouterUsage.promptTokens - cacheReadInputTokens\n\n      costOverrideDollars =\n        (openrouterUsage.cost ?? 0) +\n        (openrouterUsage.costDetails?.upstreamInferenceCost ?? 0)\n    }\n  }\n\n  saveMessage({\n    messageId,\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: options.messages,\n    response: content,\n    inputTokens,\n    outputTokens,\n    cacheCreationInputTokens,\n    cacheReadInputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n    chargeUser: options.chargeUser ?? true,\n    costOverrideDollars,\n  })\n}\n\n// TODO: figure out a nice way to unify stream & non-stream versions maybe?\nexport const promptAiSdk = async function (\n  options: {\n    messages: CodebuffMessage[]\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    model: Model\n    userId: string | undefined\n    chargeUser?: boolean\n  } & Omit<Parameters<typeof generateText>[0], 'model'>,\n): Promise<string> {\n  if (\n    !checkLiveUserInput(\n      options.userId,\n      options.userInputId,\n      options.clientSessionId,\n    )\n  ) {\n    logger.info(\n      {\n        userId: options.userId,\n        userInputId: options.userInputId,\n        liveUserInputId: getLiveUserInputIds(options.userId),\n      },\n      'Skipping prompt due to canceled user input',\n    )\n    return ''\n  }\n\n  const startTime = Date.now()\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const response = await generateText({\n    ...options,\n    model: aiSDKModel,\n  })\n\n  const content = response.text\n  const inputTokens = response.usage.inputTokens || 0\n  const outputTokens = response.usage.inputTokens || 0\n\n  saveMessage({\n    messageId: generateCompactId(),\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: options.messages,\n    response: content,\n    inputTokens,\n    outputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n    chargeUser: options.chargeUser ?? true,\n  })\n\n  return content\n}\n\n// Copied over exactly from promptAiSdk but with a schema\nexport const promptAiSdkStructured = async function <T>(options: {\n  messages: CodebuffMessage[]\n  schema: z.ZodType<T>\n  clientSessionId: string\n  fingerprintId: string\n  userInputId: string\n  model: Model\n  userId: string | undefined\n  maxTokens?: number\n  temperature?: number\n  timeout?: number\n  chargeUser?: boolean\n}): Promise<T> {\n  if (\n    !checkLiveUserInput(\n      options.userId,\n      options.userInputId,\n      options.clientSessionId,\n    )\n  ) {\n    logger.info(\n      {\n        userId: options.userId,\n        userInputId: options.userInputId,\n        liveUserInputId: getLiveUserInputIds(options.userId),\n      },\n      'Skipping structured prompt due to canceled user input',\n    )\n    return {} as T\n  }\n  const startTime = Date.now()\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const responsePromise = generateObject<z.ZodType<T>, 'object'>({\n    ...options,\n    model: aiSDKModel,\n    output: 'object',\n  })\n\n  const response = await (options.timeout === undefined\n    ? responsePromise\n    : withTimeout(responsePromise, options.timeout))\n\n  const content = response.object\n  const inputTokens = response.usage.inputTokens || 0\n  const outputTokens = response.usage.inputTokens || 0\n\n  saveMessage({\n    messageId: generateCompactId(),\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: options.messages,\n    response: JSON.stringify(content),\n    inputTokens,\n    outputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n    chargeUser: options.chargeUser ?? true,\n  })\n\n  return content\n}\n\n// TODO: temporary - ideally we move to using CodebuffMessage[] directly\n// and don't need this transform!!\nexport function transformMessages(\n  messages: (Message | CodebuffMessage)[],\n  system?: System,\n): CodebuffMessage[] {\n  const codebuffMessages: CodebuffMessage[] = []\n\n  if (system) {\n    codebuffMessages.push({\n      role: 'system',\n      content:\n        typeof system === 'string'\n          ? system\n          : system.map((block) => block.text).join('\\n\\n'),\n    })\n  }\n\n  for (const message of messages) {\n    if (message.role === 'system') {\n      if (typeof message.content === 'string') {\n        codebuffMessages.push({ role: 'system', content: message.content })\n        continue\n      } else {\n        throw new Error(\n          'Multiple part system message - unsupported (TODO: fix if we hit this.)',\n        )\n      }\n    }\n\n    if (message.role === 'user') {\n      if (typeof message.content === 'string') {\n        codebuffMessages.push({\n          ...message,\n          role: 'user',\n          content: message.content,\n        })\n        continue\n      } else {\n        const parts: UserModelMessage['content'] = []\n        const modelMessage: UserModelMessage = { role: 'user', content: parts }\n        for (const part of message.content) {\n          // Add ephemeral if present\n          if ('cache_control' in part) {\n            modelMessage.providerOptions = {\n              anthropic: { cacheControl: { type: 'ephemeral' } },\n              openrouter: { cacheControl: { type: 'ephemeral' } },\n            }\n          }\n          // Handle Message type image format\n          if (part.type === 'image' && 'source' in part) {\n            parts.push({\n              type: 'image' as const,\n              image: `data:${part.source.media_type};base64,${part.source.data}`,\n            })\n            continue\n          }\n          if (part.type === 'file') {\n            throw new Error('File messages not supported')\n          }\n          if (part.type === 'text') {\n            parts.push({\n              type: 'text' as const,\n              text: part.text,\n            })\n            continue\n          }\n          if (part.type === 'tool_use' || part.type === 'tool_result') {\n            // Skip tool parts in user messages - they should be in assistant/tool messages\n            continue\n          }\n        }\n        codebuffMessages.push(modelMessage)\n        continue\n      }\n    }\n\n    if (message.role === 'assistant') {\n      if (message.content === undefined || message.content === null) {\n        continue\n      }\n      if (typeof message.content === 'string') {\n        codebuffMessages.push({\n          ...message,\n          role: 'assistant',\n          content: message.content,\n        })\n        continue\n      } else {\n        let messageContent: AssistantModelMessage['content'] = []\n        const modelMessage: AssistantModelMessage = {\n          ...message,\n          role: 'assistant',\n          content: messageContent,\n        }\n        for (const part of message.content) {\n          // Add ephemeral if present\n          if ('cache_control' in part) {\n            modelMessage.providerOptions = {\n              anthropic: { cacheControl: { type: 'ephemeral' } },\n              openrouter: { cacheControl: { type: 'ephemeral' } },\n            }\n          }\n          if (part.type === 'text') {\n            messageContent.push({ type: 'text', text: part.text })\n          }\n          if (part.type === 'tool_use') {\n            messageContent.push({\n              type: 'tool-call',\n              toolCallId: part.id,\n              toolName: part.name,\n              input: part.input,\n            })\n          }\n        }\n        codebuffMessages.push(modelMessage)\n        continue\n      }\n    }\n\n    if (message.role === 'tool') {\n      codebuffMessages.push(message)\n      continue\n    }\n\n    throw new Error('Unknown message role received: ' + message)\n  }\n\n  return codebuffMessages\n}\n","postContent":"import { google } from '@ai-sdk/google'\nimport { openai } from '@ai-sdk/openai'\nimport {\n  finetunedVertexModels,\n  geminiModels,\n  openaiModels,\n} from '@codebuff/common/old-constants'\nimport {\n  endToolTag,\n  startToolTag,\n  toolNameParam,\n} from '@codebuff/common/tools/constants'\nimport { buildArray } from '@codebuff/common/util/array'\nimport { errorToObject } from '@codebuff/common/util/object'\nimport { withTimeout } from '@codebuff/common/util/promise'\nimport { generateCompactId } from '@codebuff/common/util/string'\nimport { APICallError, generateObject, generateText, streamText } from 'ai'\n\nimport { checkLiveUserInput, getLiveUserInputIds } from '../../live-user-inputs'\nimport { logger } from '../../util/logger'\nimport { saveMessage } from '../message-cost-tracker'\nimport { openRouterLanguageModel } from '../openrouter'\nimport { vertexFinetuned } from './vertex-finetuned'\n\nimport type { System } from '../claude'\nimport type {\n  GeminiModel,\n  Model,\n  OpenAIModel,\n} from '@codebuff/common/old-constants'\nimport type { CodebuffMessage, Message } from '@codebuff/common/types/message'\nimport type { OpenRouterUsageAccounting } from '@openrouter/ai-sdk-provider'\nimport type { AssistantModelMessage, UserModelMessage, LanguageModel } from 'ai'\nimport type { z } from 'zod/v4'\n\n// TODO: We'll want to add all our models here!\nconst modelToAiSDKModel = (model: Model): LanguageModel => {\n  if (\n    Object.values(finetunedVertexModels as Record<string, string>).includes(\n      model,\n    )\n  ) {\n    return vertexFinetuned(model)\n  }\n  if (Object.values(geminiModels).includes(model as GeminiModel)) {\n    return google.languageModel(model)\n  }\n  if (model === openaiModels.o3pro || model === openaiModels.o3) {\n    return openai.responses(model)\n  }\n  if (Object.values(openaiModels).includes(model as OpenAIModel)) {\n    return openai.languageModel(model)\n  }\n  // All other models go through OpenRouter\n  return openRouterLanguageModel(model)\n}\n\n// TODO: Add retries & fallbacks: likely by allowing this to instead of \"model\"\n// also take an array of form [{model: Model, retries: number}, {model: Model, retries: number}...]\n// eg: [{model: \"gemini-2.0-flash-001\"}, {model: \"vertex/gemini-2.0-flash-001\"}, {model: \"claude-3-5-haiku\", retries: 3}]\nexport const promptAiSdkStream = async function* (\n  options: {\n    messages: CodebuffMessage[]\n    clientSessionId: string\n    fingerprintId: string\n    model: Model\n    userId: string | undefined\n    chargeUser?: boolean\n    thinkingBudget?: number\n    userInputId: string\n    maxRetries?: number\n  } & Omit<Parameters<typeof streamText>[0], 'model'>,\n) {\n  if (\n    !checkLiveUserInput(\n      options.userId,\n      options.userInputId,\n      options.clientSessionId,\n    )\n  ) {\n    logger.info(\n      {\n        userId: options.userId,\n        userInputId: options.userInputId,\n        liveUserInputId: getLiveUserInputIds(options.userId),\n      },\n      'Skipping stream due to canceled user input',\n    )\n    yield ''\n    return\n  }\n  const startTime = Date.now()\n\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const response = streamText({\n    ...options,\n    model: aiSDKModel,\n    maxRetries: options.maxRetries,\n  })\n\n  let content = ''\n  let reasoning = false\n\n  for await (const chunk of response.fullStream) {\n    if (chunk.type === 'error') {\n      logger.error(\n        {\n          chunk: { ...chunk, error: undefined },\n          error: errorToObject(chunk.error),\n          model: options.model,\n        },\n        'Error from AI SDK',\n      )\n\n      const errorBody = APICallError.isInstance(chunk.error)\n        ? chunk.error.responseBody\n        : undefined\n      const mainErrorMessage =\n        chunk.error instanceof Error\n          ? chunk.error.message\n          : typeof chunk.error === 'string'\n            ? chunk.error\n            : JSON.stringify(chunk.error)\n      const errorMessage = `Error from AI SDK (model ${options.model}): ${buildArray([mainErrorMessage, errorBody]).join('\\n')}`\n      throw new Error(errorMessage, {\n        cause: chunk.error,\n      })\n    }\n    if (chunk.type === 'reasoning-delta') {\n      if (!reasoning) {\n        reasoning = true\n        yield `${startToolTag}{\n  ${JSON.stringify(toolNameParam)}: \"think_deeply\",\n  \"thought\": \"`\n      }\n      yield JSON.stringify(chunk.text).slice(1, -1)\n    }\n    if (chunk.type === 'text-delta') {\n      if (reasoning) {\n        reasoning = false\n        yield `\"\\n}${endToolTag}\\n\\n`\n      }\n      content += chunk.text\n      yield chunk.text\n    }\n  }\n\n  const messageId = (await response.response).id\n  const providerMetadata = (await response.providerMetadata) ?? {}\n  const usage = await response.usage\n  let inputTokens = usage.inputTokens || 0\n  const outputTokens = usage.outputTokens || 0\n  let cacheReadInputTokens: number = 0\n  let cacheCreationInputTokens: number = 0\n  let costOverrideDollars: number | undefined\n  if (providerMetadata.anthropic) {\n    cacheReadInputTokens =\n      typeof providerMetadata.anthropic.cacheReadInputTokens === 'number'\n        ? providerMetadata.anthropic.cacheReadInputTokens\n        : 0\n    cacheCreationInputTokens =\n      typeof providerMetadata.anthropic.cacheCreationInputTokens === 'number'\n        ? providerMetadata.anthropic.cacheCreationInputTokens\n        : 0\n  }\n  if (providerMetadata.openrouter) {\n    if (providerMetadata.openrouter.usage) {\n      const openrouterUsage = providerMetadata.openrouter\n        .usage as OpenRouterUsageAccounting\n      cacheReadInputTokens =\n        openrouterUsage.promptTokensDetails?.cachedTokens ?? 0\n      inputTokens = openrouterUsage.promptTokens - cacheReadInputTokens\n\n      costOverrideDollars =\n        (openrouterUsage.cost ?? 0) +\n        (openrouterUsage.costDetails?.upstreamInferenceCost ?? 0)\n    }\n  }\n\n  saveMessage({\n    messageId,\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: options.messages,\n    response: content,\n    inputTokens,\n    outputTokens,\n    cacheCreationInputTokens,\n    cacheReadInputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n    chargeUser: options.chargeUser ?? true,\n    costOverrideDollars,\n  })\n}\n\n// TODO: figure out a nice way to unify stream & non-stream versions maybe?\nexport const promptAiSdk = async function (\n  options: {\n    messages: CodebuffMessage[]\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    model: Model\n    userId: string | undefined\n    chargeUser?: boolean\n  } & Omit<Parameters<typeof generateText>[0], 'model'>,\n): Promise<string> {\n  if (\n    !checkLiveUserInput(\n      options.userId,\n      options.userInputId,\n      options.clientSessionId,\n    )\n  ) {\n    logger.info(\n      {\n        userId: options.userId,\n        userInputId: options.userInputId,\n        liveUserInputId: getLiveUserInputIds(options.userId),\n      },\n      'Skipping prompt due to canceled user input',\n    )\n    return ''\n  }\n\n  const startTime = Date.now()\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const response = await generateText({\n    ...options,\n    model: aiSDKModel,\n  })\n\n  const content = response.text\n  const inputTokens = response.usage.inputTokens || 0\n  const outputTokens = response.usage.inputTokens || 0\n\n  saveMessage({\n    messageId: generateCompactId(),\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: options.messages,\n    response: content,\n    inputTokens,\n    outputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n    chargeUser: options.chargeUser ?? true,\n  })\n\n  return content\n}\n\n// Copied over exactly from promptAiSdk but with a schema\nexport const promptAiSdkStructured = async function <T>(options: {\n  messages: CodebuffMessage[]\n  schema: z.ZodType<T>\n  clientSessionId: string\n  fingerprintId: string\n  userInputId: string\n  model: Model\n  userId: string | undefined\n  maxTokens?: number\n  temperature?: number\n  timeout?: number\n  chargeUser?: boolean\n}): Promise<T> {\n  if (\n    !checkLiveUserInput(\n      options.userId,\n      options.userInputId,\n      options.clientSessionId,\n    )\n  ) {\n    logger.info(\n      {\n        userId: options.userId,\n        userInputId: options.userInputId,\n        liveUserInputId: getLiveUserInputIds(options.userId),\n      },\n      'Skipping structured prompt due to canceled user input',\n    )\n    return {} as T\n  }\n  const startTime = Date.now()\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const responsePromise = generateObject<z.ZodType<T>, 'object'>({\n    ...options,\n    model: aiSDKModel,\n    output: 'object',\n  })\n\n  const response = await (options.timeout === undefined\n    ? responsePromise\n    : withTimeout(responsePromise, options.timeout))\n\n  const content = response.object\n  const inputTokens = response.usage.inputTokens || 0\n  const outputTokens = response.usage.inputTokens || 0\n\n  saveMessage({\n    messageId: generateCompactId(),\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: options.messages,\n    response: JSON.stringify(content),\n    inputTokens,\n    outputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n    chargeUser: options.chargeUser ?? true,\n  })\n\n  return content\n}\n\n// TODO: temporary - ideally we move to using CodebuffMessage[] directly\n// and don't need this transform!!\nexport function transformMessages(\n  messages: (Message | CodebuffMessage)[],\n  system?: System,\n): CodebuffMessage[] {\n  const codebuffMessages: CodebuffMessage[] = []\n\n  if (system) {\n    codebuffMessages.push({\n      role: 'system',\n      content:\n        typeof system === 'string'\n          ? system\n          : system.map((block) => block.text).join('\\n\\n'),\n    })\n  }\n\n  for (const message of messages) {\n    if (message.role === 'system') {\n      if (typeof message.content === 'string') {\n        codebuffMessages.push({ role: 'system', content: message.content })\n        continue\n      } else {\n        throw new Error(\n          'Multiple part system message - unsupported (TODO: fix if we hit this.)',\n        )\n      }\n    }\n\n    if (message.role === 'user') {\n      if (typeof message.content === 'string') {\n        codebuffMessages.push({\n          ...message,\n          role: 'user',\n          content: message.content,\n        })\n        continue\n      } else {\n        const parts: UserModelMessage['content'] = []\n        const modelMessage: UserModelMessage = { role: 'user', content: parts }\n        for (const part of message.content) {\n          // Add ephemeral if present\n          if ('cache_control' in part) {\n            modelMessage.providerOptions = {\n              anthropic: { cacheControl: { type: 'ephemeral' } },\n              openrouter: { cacheControl: { type: 'ephemeral' } },\n            }\n          }\n          // Handle Message type image format\n          if (part.type === 'image' && 'source' in part) {\n            parts.push({\n              type: 'image' as const,\n              image: `data:${part.source.media_type};base64,${part.source.data}`,\n            })\n            continue\n          }\n          if (part.type === 'file') {\n            throw new Error('File messages not supported')\n          }\n          if (part.type === 'text') {\n            parts.push({\n              type: 'text' as const,\n              text: part.text,\n            })\n            continue\n          }\n          if (part.type === 'tool_use' || part.type === 'tool_result') {\n            // Skip tool parts in user messages - they should be in assistant/tool messages\n            continue\n          }\n        }\n        codebuffMessages.push(modelMessage)\n        continue\n      }\n    }\n\n    if (message.role === 'assistant') {\n      if (message.content === undefined || message.content === null) {\n        continue\n      }\n      if (typeof message.content === 'string') {\n        codebuffMessages.push({\n          ...message,\n          role: 'assistant',\n          content: message.content,\n        })\n        continue\n      } else {\n        let messageContent: AssistantModelMessage['content'] = []\n        const modelMessage: AssistantModelMessage = {\n          ...message,\n          role: 'assistant',\n          content: messageContent,\n        }\n        for (const part of message.content) {\n          // Add ephemeral if present\n          if ('cache_control' in part) {\n            modelMessage.providerOptions = {\n              anthropic: { cacheControl: { type: 'ephemeral' } },\n              openrouter: { cacheControl: { type: 'ephemeral' } },\n            }\n          }\n          if (part.type === 'text') {\n            messageContent.push({ type: 'text', text: part.text })\n          }\n          if (part.type === 'tool_use') {\n            messageContent.push({\n              type: 'tool-call',\n              toolCallId: part.id,\n              toolName: part.name,\n              input: part.input,\n            })\n          }\n        }\n        codebuffMessages.push(modelMessage)\n        continue\n      }\n    }\n\n    if (message.role === 'tool') {\n      codebuffMessages.push(message)\n      continue\n    }\n\n    throw new Error('Unknown message role received: ' + message)\n  }\n\n  return codebuffMessages\n}\n"},{"path":"backend/src/prompt-agent-stream.ts","preContent":"import { providerModelNames } from '@codebuff/common/old-constants'\n\nimport { promptAiSdkStream } from './llm-apis/vercel-ai-sdk/ai-sdk'\nimport { globalStopSequence } from './tools/constants'\n\nimport type { AgentTemplate } from './templates/types'\nimport type { CodebuffMessage } from '@codebuff/common/types/message'\n\nexport const getAgentStreamFromTemplate = (params: {\n  clientSessionId: string\n  fingerprintId: string\n  userInputId: string\n  userId: string | undefined\n\n  template: AgentTemplate\n}) => {\n  const { clientSessionId, fingerprintId, userInputId, userId, template } =\n    params\n\n  if (!template) {\n    throw new Error('Agent template is null/undefined')\n  }\n\n  const { model } = template\n\n  const getStream = (messages: CodebuffMessage[]) => {\n    const options: Parameters<typeof promptAiSdkStream>[0] = {\n      messages,\n      model,\n      stopSequences: [globalStopSequence],\n      clientSessionId,\n      fingerprintId,\n      userInputId,\n      userId,\n      maxOutputTokens: 32_000,\n    }\n\n    // Add Gemini-specific options if needed\n    const primaryModel = Array.isArray(model) ? model[0] : model\n    const provider =\n      providerModelNames[primaryModel as keyof typeof providerModelNames]\n\n    if (provider === 'gemini') {\n      if (!options.providerOptions) {\n        options.providerOptions = {}\n      }\n      if (!options.providerOptions.gemini) {\n        options.providerOptions.gemini = {}\n      }\n      if (!options.providerOptions.gemini.thinkingConfig) {\n        options.providerOptions.gemini.thinkingConfig = { thinkingBudget: 128 }\n      }\n    }\n\n    return promptAiSdkStream(options)\n  }\n\n  return getStream\n}\n","postContent":"import { providerModelNames } from '@codebuff/common/old-constants'\n\nimport { promptAiSdkStream } from './llm-apis/vercel-ai-sdk/ai-sdk'\nimport { globalStopSequence } from './tools/constants'\n\nimport type { AgentTemplate } from './templates/types'\nimport type { CodebuffMessage } from '@codebuff/common/types/message'\nimport type { OpenRouterProviderOptions } from '@codebuff/internal/openrouter-ai-sdk'\n\nexport const getAgentStreamFromTemplate = (params: {\n  clientSessionId: string\n  fingerprintId: string\n  userInputId: string\n  userId: string | undefined\n\n  template: AgentTemplate\n}) => {\n  const { clientSessionId, fingerprintId, userInputId, userId, template } =\n    params\n\n  if (!template) {\n    throw new Error('Agent template is null/undefined')\n  }\n\n  const { model } = template\n\n  const getStream = (messages: CodebuffMessage[]) => {\n    const options: Parameters<typeof promptAiSdkStream>[0] = {\n      messages,\n      model,\n      stopSequences: [globalStopSequence],\n      clientSessionId,\n      fingerprintId,\n      userInputId,\n      userId,\n      maxOutputTokens: 32_000,\n    }\n\n    // Add Gemini-specific options if needed\n    const primaryModel = Array.isArray(model) ? model[0] : model\n    const provider =\n      providerModelNames[primaryModel as keyof typeof providerModelNames]\n\n    if (!options.providerOptions) {\n      options.providerOptions = {}\n    }\n    if (provider === 'gemini') {\n      if (!options.providerOptions.gemini) {\n        options.providerOptions.gemini = {}\n      }\n      if (!options.providerOptions.gemini.thinkingConfig) {\n        options.providerOptions.gemini.thinkingConfig = { thinkingBudget: 128 }\n      }\n    }\n    if (!options.providerOptions.openrouter) {\n      options.providerOptions.openrouter = {}\n    }\n    ;(\n      options.providerOptions.openrouter as OpenRouterProviderOptions\n    ).reasoning = template.reasoningOptions\n\n    return promptAiSdkStream(options)\n  }\n\n  return getStream\n}\n"},{"path":"common/src/templates/initial-agents-dir/types/agent-definition.ts","preContent":"/**\n * Codebuff Agent Type Definitions\n *\n * This file provides TypeScript type definitions for creating custom Codebuff agents.\n * Import these types in your agent files to get full type safety and IntelliSense.\n *\n * Usage in .agents/your-agent.ts:\n *   import { AgentDefinition, ToolName, ModelName } from './types/agent-definition'\n *\n *   const definition: AgentDefinition = {\n *     // ... your agent configuration with full type safety ...\n *   }\n *\n *   export default definition\n */\n\n// ============================================================================\n// Agent Definition and Utility Types\n// ============================================================================\n\nexport interface AgentDefinition {\n  /** Unique identifier for this agent. Must contain only lowercase letters, numbers, and hyphens, e.g. 'code-reviewer' */\n  id: string\n\n  /** Version string (if not provided, will default to '0.0.1' and be bumped on each publish) */\n  version?: string\n\n  /** Publisher ID for the agent. Must be provided if you want to publish the agent. */\n  publisher?: string\n\n  /** Human-readable name for the agent */\n  displayName: string\n\n  /** AI model to use for this agent. Can be any model in OpenRouter: https://openrouter.ai/models */\n  model: ModelName\n\n  // ============================================================================\n  // Tools and Subagents\n  // ============================================================================\n\n  /** Tools this agent can use. */\n  toolNames?: ToolName[]\n\n  /** Other agents this agent can spawn, like 'codebuff/file-picker@0.0.1'.\n   *\n   * Use the fully qualified agent id from the agent store, including publisher and version: 'codebuff/file-picker@0.0.1'\n   * (publisher and version are required!)\n   *\n   * Or, use the agent id from a local agent file in your .agents directory: 'file-picker'.\n   */\n  spawnableAgents?: string[]\n\n  // ============================================================================\n  // Input and Output\n  // ============================================================================\n\n  /** The input schema required to spawn the agent. Provide a prompt string and/or a params object or none.\n   * 80% of the time you want just a prompt string with a description:\n   * inputSchema: {\n   *   prompt: { type: 'string', description: 'A description of what info would be helpful to the agent' }\n   * }\n   */\n  inputSchema?: {\n    prompt?: { type: 'string'; description?: string }\n    params?: JsonObjectSchema\n  }\n\n  /** Whether to include conversation history from the parent agent in context.\n   *\n   * Defaults to false.\n   * Use this if the agent needs to know all the previous messages in the conversation.\n   */\n  includeMessageHistory?: boolean\n\n  /** How the agent should output a response to its parent (defaults to 'last_message')\n   *\n   * last_message: The last message from the agent, typcically after using tools.\n   *\n   * all_messages: All messages from the agent, including tool calls and results.\n   *\n   * json: Make the agent output a JSON object. Can be used with outputSchema or without if you want freeform json output.\n   */\n  outputMode?: 'last_message' | 'all_messages' | 'structured_output'\n\n  /** JSON schema for structured output (when outputMode is 'structured_output') */\n  outputSchema?: JsonObjectSchema\n\n  // ============================================================================\n  // Prompts\n  // ============================================================================\n\n  /** Prompt for when and why to spawn this agent. Include the main purpose and use cases.\n   *\n   * This field is key if the agent is intended to be spawned by other agents. */\n  spawnerPrompt?: string\n\n  /** Background information for the agent. Fairly optional. Prefer using instructionsPrompt for agent instructions. */\n  systemPrompt?: string\n\n  /** Instructions for the agent.\n   *\n   * IMPORTANT: Updating this prompt is the best way to shape the agent's behavior.\n   * This prompt is inserted after each user input. */\n  instructionsPrompt?: string\n\n  /** Prompt inserted at each agent step.\n   *\n   * Powerful for changing the agent's behavior, but usually not necessary for smart models.\n   * Prefer instructionsPrompt for most instructions. */\n  stepPrompt?: string\n\n  // ============================================================================\n  // Handle Steps\n  // ============================================================================\n\n  /** Programmatically step the agent forward and run tools.\n   *\n   * You can either yield:\n   * - A tool call object with toolName and input properties.\n   * - 'STEP' to run agent's model and generate one assistant message.\n   * - 'STEP_ALL' to run the agent's model until it uses the end_turn tool or stops includes no tool calls in a message.\n   *\n   * Or use 'return' to end the turn.\n   *\n   * Example 1:\n   * function* handleSteps({ agentStep, prompt, params}) {\n   *   const { toolResult } = yield {\n   *     toolName: 'read_files',\n   *     input: { paths: ['file1.txt', 'file2.txt'] }\n   *   }\n   *   yield 'STEP_ALL'\n   * }\n   *\n   * Example 2:\n   * handleSteps: function* ({ agentState, prompt, params }) {\n   *   while (true) {\n   *     yield {\n   *       toolName: 'spawn_agents',\n   *       input: {\n   *         agents: [\n   *         {\n   *           agent_type: 'thinker',\n   *           prompt: 'Think deeply about the user request',\n   *         },\n   *       ],\n   *     },\n   *   }\n   *   yield 'STEP'\n   * }\n   * }\n   */\n  handleSteps?: (\n    context: AgentStepContext,\n  ) => Generator<\n    ToolCall | 'STEP' | 'STEP_ALL',\n    void,\n    { agentState: AgentState; toolResult: string | undefined }\n  >\n}\n\n// ============================================================================\n// Supporting Types\n// ============================================================================\n\nexport interface AgentState {\n  agentId: string\n  parentId: string\n  messageHistory: Message[]\n}\n\n/**\n * Message in conversation history\n */\nexport interface Message {\n  role: 'user' | 'assistant'\n  content: string\n}\n\n/**\n * Context provided to handleSteps generator function\n */\nexport interface AgentStepContext {\n  agentState: AgentState\n  prompt?: string\n  params?: Record<string, any>\n}\n\n/**\n * Tool call object for handleSteps generator\n */\nexport type ToolCall<T extends ToolName = ToolName> = {\n  [K in T]: {\n    toolName: K\n    input: Tools.GetToolParams<K>\n  }\n}[T]\n\n/**\n * JSON Schema definition (for prompt schema or output schema)\n */\nexport type JsonSchema = {\n  type?:\n    | 'object'\n    | 'array'\n    | 'string'\n    | 'number'\n    | 'boolean'\n    | 'null'\n    | 'integer'\n  description?: string\n  properties?: Record<string, JsonSchema | boolean>\n  required?: string[]\n  enum?: Array<string | number | boolean | null>\n  [k: string]: unknown\n}\nexport type JsonObjectSchema = JsonSchema & { type: 'object' }\n\n// ============================================================================\n// Available Tools\n// ============================================================================\n\n/**\n * File operation tools\n */\nexport type FileTools =\n  | 'read_files'\n  | 'write_file'\n  | 'str_replace'\n  | 'find_files'\n\n/**\n * Code analysis tools\n */\nexport type CodeAnalysisTools = 'code_search' | 'find_files'\n\n/**\n * Terminal and system tools\n */\nexport type TerminalTools = 'run_terminal_command' | 'run_file_change_hooks'\n\n/**\n * Web and browser tools\n */\nexport type WebTools = 'web_search' | 'read_docs'\n\n/**\n * Agent management tools\n */\nexport type AgentTools = 'spawn_agents' | 'set_messages' | 'add_message'\n\n/**\n * Planning and organization tools\n */\nexport type PlanningTools = 'think_deeply'\n\n/**\n * Output and control tools\n */\nexport type OutputTools = 'set_output' | 'end_turn'\n\n/**\n * Common tool combinations for convenience\n */\nexport type FileEditingTools = FileTools | 'end_turn'\nexport type ResearchTools = WebTools | 'write_file' | 'end_turn'\nexport type CodeAnalysisToolSet = FileTools | CodeAnalysisTools | 'end_turn'\n\n// ============================================================================\n// Available Models (see: https://openrouter.ai/models)\n// ============================================================================\n\n/**\n * AI models available for agents. Pick from our selection of recommended models or choose any model in OpenRouter.\n *\n * See available models at https://openrouter.ai/models\n */\nexport type ModelName =\n  // Recommended Models\n\n  // OpenAI\n  | 'openai/gpt-5'\n  | 'openai/gpt-5-mini'\n  | 'openai/gpt-5-nano'\n\n  // Anthropic\n  | 'anthropic/claude-4-sonnet-20250522'\n  | 'anthropic/claude-opus-4.1'\n\n  // Gemini\n  | 'google/gemini-2.5-pro'\n  | 'google/gemini-2.5-flash'\n  | 'google/gemini-2.5-flash-lite'\n\n  // X-AI\n  | 'x-ai/grok-4-07-09'\n\n  // Qwen\n  | 'qwen/qwen3-coder'\n  | 'qwen/qwen3-coder:nitro'\n  | 'qwen/qwen3-235b-a22b-2507'\n  | 'qwen/qwen3-235b-a22b-2507:nitro'\n  | 'qwen/qwen3-235b-a22b-thinking-2507'\n  | 'qwen/qwen3-235b-a22b-thinking-2507:nitro'\n  | 'qwen/qwen3-30b-a3b'\n  | 'qwen/qwen3-30b-a3b:nitro'\n\n  // DeepSeek\n  | 'deepseek/deepseek-chat-v3-0324'\n  | 'deepseek/deepseek-chat-v3-0324:nitro'\n  | 'deepseek/deepseek-r1-0528'\n  | 'deepseek/deepseek-r1-0528:nitro'\n\n  // Other open source models\n  | 'moonshotai/kimi-k2'\n  | 'moonshotai/kimi-k2:nitro'\n  | 'z-ai/glm-4.5'\n  | 'z-ai/glm-4.5:nitro'\n  | (string & {})\n\nimport type * as Tools from './tools'\nexport type { Tools }\ntype ToolName = Tools.ToolName\n","postContent":"/**\n * Codebuff Agent Type Definitions\n *\n * This file provides TypeScript type definitions for creating custom Codebuff agents.\n * Import these types in your agent files to get full type safety and IntelliSense.\n *\n * Usage in .agents/your-agent.ts:\n *   import { AgentDefinition, ToolName, ModelName } from './types/agent-definition'\n *\n *   const definition: AgentDefinition = {\n *     // ... your agent configuration with full type safety ...\n *   }\n *\n *   export default definition\n */\n\n// ============================================================================\n// Agent Definition and Utility Types\n// ============================================================================\n\nexport interface AgentDefinition {\n  /** Unique identifier for this agent. Must contain only lowercase letters, numbers, and hyphens, e.g. 'code-reviewer' */\n  id: string\n\n  /** Version string (if not provided, will default to '0.0.1' and be bumped on each publish) */\n  version?: string\n\n  /** Publisher ID for the agent. Must be provided if you want to publish the agent. */\n  publisher?: string\n\n  /** Human-readable name for the agent */\n  displayName: string\n\n  /** AI model to use for this agent. Can be any model in OpenRouter: https://openrouter.ai/models */\n  model: ModelName\n\n  /**\n   * https://openrouter.ai/docs/use-cases/reasoning-tokens\n   * One of `max_tokens` or `effort` is required.\n   * If `exclude` is true, reasoning will be removed from the response. Default is false.\n   */\n  reasoningOptions?: {\n    enabled?: boolean\n    exclude?: boolean\n  } & (\n    | {\n        max_tokens: number\n      }\n    | {\n        effort: 'high' | 'medium' | 'low'\n      }\n  )\n\n  // ============================================================================\n  // Tools and Subagents\n  // ============================================================================\n\n  /** Tools this agent can use. */\n  toolNames?: ToolName[]\n\n  /** Other agents this agent can spawn, like 'codebuff/file-picker@0.0.1'.\n   *\n   * Use the fully qualified agent id from the agent store, including publisher and version: 'codebuff/file-picker@0.0.1'\n   * (publisher and version are required!)\n   *\n   * Or, use the agent id from a local agent file in your .agents directory: 'file-picker'.\n   */\n  spawnableAgents?: string[]\n\n  // ============================================================================\n  // Input and Output\n  // ============================================================================\n\n  /** The input schema required to spawn the agent. Provide a prompt string and/or a params object or none.\n   * 80% of the time you want just a prompt string with a description:\n   * inputSchema: {\n   *   prompt: { type: 'string', description: 'A description of what info would be helpful to the agent' }\n   * }\n   */\n  inputSchema?: {\n    prompt?: { type: 'string'; description?: string }\n    params?: JsonObjectSchema\n  }\n\n  /** Whether to include conversation history from the parent agent in context.\n   *\n   * Defaults to false.\n   * Use this if the agent needs to know all the previous messages in the conversation.\n   */\n  includeMessageHistory?: boolean\n\n  /** How the agent should output a response to its parent (defaults to 'last_message')\n   *\n   * last_message: The last message from the agent, typcically after using tools.\n   *\n   * all_messages: All messages from the agent, including tool calls and results.\n   *\n   * json: Make the agent output a JSON object. Can be used with outputSchema or without if you want freeform json output.\n   */\n  outputMode?: 'last_message' | 'all_messages' | 'structured_output'\n\n  /** JSON schema for structured output (when outputMode is 'structured_output') */\n  outputSchema?: JsonObjectSchema\n\n  // ============================================================================\n  // Prompts\n  // ============================================================================\n\n  /** Prompt for when and why to spawn this agent. Include the main purpose and use cases.\n   *\n   * This field is key if the agent is intended to be spawned by other agents. */\n  spawnerPrompt?: string\n\n  /** Background information for the agent. Fairly optional. Prefer using instructionsPrompt for agent instructions. */\n  systemPrompt?: string\n\n  /** Instructions for the agent.\n   *\n   * IMPORTANT: Updating this prompt is the best way to shape the agent's behavior.\n   * This prompt is inserted after each user input. */\n  instructionsPrompt?: string\n\n  /** Prompt inserted at each agent step.\n   *\n   * Powerful for changing the agent's behavior, but usually not necessary for smart models.\n   * Prefer instructionsPrompt for most instructions. */\n  stepPrompt?: string\n\n  // ============================================================================\n  // Handle Steps\n  // ============================================================================\n\n  /** Programmatically step the agent forward and run tools.\n   *\n   * You can either yield:\n   * - A tool call object with toolName and input properties.\n   * - 'STEP' to run agent's model and generate one assistant message.\n   * - 'STEP_ALL' to run the agent's model until it uses the end_turn tool or stops includes no tool calls in a message.\n   *\n   * Or use 'return' to end the turn.\n   *\n   * Example 1:\n   * function* handleSteps({ agentStep, prompt, params}) {\n   *   const { toolResult } = yield {\n   *     toolName: 'read_files',\n   *     input: { paths: ['file1.txt', 'file2.txt'] }\n   *   }\n   *   yield 'STEP_ALL'\n   * }\n   *\n   * Example 2:\n   * handleSteps: function* ({ agentState, prompt, params }) {\n   *   while (true) {\n   *     yield {\n   *       toolName: 'spawn_agents',\n   *       input: {\n   *         agents: [\n   *         {\n   *           agent_type: 'thinker',\n   *           prompt: 'Think deeply about the user request',\n   *         },\n   *       ],\n   *     },\n   *   }\n   *   yield 'STEP'\n   * }\n   * }\n   */\n  handleSteps?: (\n    context: AgentStepContext,\n  ) => Generator<\n    ToolCall | 'STEP' | 'STEP_ALL',\n    void,\n    { agentState: AgentState; toolResult: string | undefined }\n  >\n}\n\n// ============================================================================\n// Supporting Types\n// ============================================================================\n\nexport interface AgentState {\n  agentId: string\n  parentId: string\n  messageHistory: Message[]\n}\n\n/**\n * Message in conversation history\n */\nexport interface Message {\n  role: 'user' | 'assistant'\n  content: string\n}\n\n/**\n * Context provided to handleSteps generator function\n */\nexport interface AgentStepContext {\n  agentState: AgentState\n  prompt?: string\n  params?: Record<string, any>\n}\n\n/**\n * Tool call object for handleSteps generator\n */\nexport type ToolCall<T extends ToolName = ToolName> = {\n  [K in T]: {\n    toolName: K\n    input: Tools.GetToolParams<K>\n  }\n}[T]\n\n/**\n * JSON Schema definition (for prompt schema or output schema)\n */\nexport type JsonSchema = {\n  type?:\n    | 'object'\n    | 'array'\n    | 'string'\n    | 'number'\n    | 'boolean'\n    | 'null'\n    | 'integer'\n  description?: string\n  properties?: Record<string, JsonSchema | boolean>\n  required?: string[]\n  enum?: Array<string | number | boolean | null>\n  [k: string]: unknown\n}\nexport type JsonObjectSchema = JsonSchema & { type: 'object' }\n\n// ============================================================================\n// Available Tools\n// ============================================================================\n\n/**\n * File operation tools\n */\nexport type FileTools =\n  | 'read_files'\n  | 'write_file'\n  | 'str_replace'\n  | 'find_files'\n\n/**\n * Code analysis tools\n */\nexport type CodeAnalysisTools = 'code_search' | 'find_files'\n\n/**\n * Terminal and system tools\n */\nexport type TerminalTools = 'run_terminal_command' | 'run_file_change_hooks'\n\n/**\n * Web and browser tools\n */\nexport type WebTools = 'web_search' | 'read_docs'\n\n/**\n * Agent management tools\n */\nexport type AgentTools = 'spawn_agents' | 'set_messages' | 'add_message'\n\n/**\n * Planning and organization tools\n */\nexport type PlanningTools = 'think_deeply'\n\n/**\n * Output and control tools\n */\nexport type OutputTools = 'set_output' | 'end_turn'\n\n/**\n * Common tool combinations for convenience\n */\nexport type FileEditingTools = FileTools | 'end_turn'\nexport type ResearchTools = WebTools | 'write_file' | 'end_turn'\nexport type CodeAnalysisToolSet = FileTools | CodeAnalysisTools | 'end_turn'\n\n// ============================================================================\n// Available Models (see: https://openrouter.ai/models)\n// ============================================================================\n\n/**\n * AI models available for agents. Pick from our selection of recommended models or choose any model in OpenRouter.\n *\n * See available models at https://openrouter.ai/models\n */\nexport type ModelName =\n  // Recommended Models\n\n  // OpenAI\n  | 'openai/gpt-5'\n  | 'openai/gpt-5-mini'\n  | 'openai/gpt-5-nano'\n\n  // Anthropic\n  | 'anthropic/claude-4-sonnet-20250522'\n  | 'anthropic/claude-opus-4.1'\n\n  // Gemini\n  | 'google/gemini-2.5-pro'\n  | 'google/gemini-2.5-flash'\n  | 'google/gemini-2.5-flash-lite'\n\n  // X-AI\n  | 'x-ai/grok-4-07-09'\n\n  // Qwen\n  | 'qwen/qwen3-coder'\n  | 'qwen/qwen3-coder:nitro'\n  | 'qwen/qwen3-235b-a22b-2507'\n  | 'qwen/qwen3-235b-a22b-2507:nitro'\n  | 'qwen/qwen3-235b-a22b-thinking-2507'\n  | 'qwen/qwen3-235b-a22b-thinking-2507:nitro'\n  | 'qwen/qwen3-30b-a3b'\n  | 'qwen/qwen3-30b-a3b:nitro'\n\n  // DeepSeek\n  | 'deepseek/deepseek-chat-v3-0324'\n  | 'deepseek/deepseek-chat-v3-0324:nitro'\n  | 'deepseek/deepseek-r1-0528'\n  | 'deepseek/deepseek-r1-0528:nitro'\n\n  // Other open source models\n  | 'moonshotai/kimi-k2'\n  | 'moonshotai/kimi-k2:nitro'\n  | 'z-ai/glm-4.5'\n  | 'z-ai/glm-4.5:nitro'\n  | (string & {})\n\nimport type * as Tools from './tools'\nexport type { Tools }\ntype ToolName = Tools.ToolName\n"},{"path":"common/src/types/agent-template.ts","preContent":"import type { Model } from '../constants'\nimport type { AgentState, AgentTemplateType } from './session-state'\nimport type { ToolCall } from '../templates/initial-agents-dir/types/agent-definition'\nimport type { ToolName } from '../tools/constants'\nimport type { z } from 'zod/v4'\n\nexport type AgentTemplate<\n  P = string | undefined,\n  T = Record<string, any> | undefined,\n> = {\n  id: AgentTemplateType\n  displayName: string\n  model: Model\n\n  toolNames: ToolName[]\n  spawnableAgents: AgentTemplateType[]\n\n  spawnerPrompt?: string\n  systemPrompt: string\n  instructionsPrompt: string\n  stepPrompt: string\n  parentInstructions?: Record<string, string>\n\n  // Required parameters for spawning this agent.\n  inputSchema: {\n    prompt?: z.ZodSchema<P>\n    params?: z.ZodSchema<T>\n  }\n  includeMessageHistory: boolean\n  outputMode: 'last_message' | 'all_messages' | 'structured_output'\n  outputSchema?: z.ZodSchema<any>\n\n  handleSteps?: StepHandler<P, T> | string // Function or string of the generator code for running in a sandbox\n}\n\nexport type StepGenerator = Generator<\n  Omit<ToolCall, 'toolCallId'> | 'STEP' | 'STEP_ALL', // Generic tool call type\n  void,\n  { agentState: AgentState; toolResult: string | undefined }\n>\n\nexport type StepHandler<\n  P = string | undefined,\n  T = Record<string, any> | undefined,\n> = (params: { agentState: AgentState; prompt: P; params: T }) => StepGenerator\n","postContent":"import type { Model } from '../constants'\nimport type { AgentState, AgentTemplateType } from './session-state'\nimport type { ToolCall } from '../templates/initial-agents-dir/types/agent-definition'\nimport type { ToolName } from '../tools/constants'\nimport type { OpenRouterProviderOptions } from '@codebuff/internal/openrouter-ai-sdk'\nimport type { z } from 'zod/v4'\n\nexport type AgentTemplate<\n  P = string | undefined,\n  T = Record<string, any> | undefined,\n> = {\n  id: AgentTemplateType\n  displayName: string\n  model: Model\n  reasoningOptions: OpenRouterProviderOptions['reasoning']\n\n  toolNames: ToolName[]\n  spawnableAgents: AgentTemplateType[]\n\n  spawnerPrompt?: string\n  systemPrompt: string\n  instructionsPrompt: string\n  stepPrompt: string\n  parentInstructions?: Record<string, string>\n\n  // Required parameters for spawning this agent.\n  inputSchema: {\n    prompt?: z.ZodSchema<P>\n    params?: z.ZodSchema<T>\n  }\n  includeMessageHistory: boolean\n  outputMode: 'last_message' | 'all_messages' | 'structured_output'\n  outputSchema?: z.ZodSchema<any>\n\n  handleSteps?: StepHandler<P, T> | string // Function or string of the generator code for running in a sandbox\n}\n\nexport type StepGenerator = Generator<\n  Omit<ToolCall, 'toolCallId'> | 'STEP' | 'STEP_ALL', // Generic tool call type\n  void,\n  { agentState: AgentState; toolResult: string | undefined }\n>\n\nexport type StepHandler<\n  P = string | undefined,\n  T = Record<string, any> | undefined,\n> = (params: { agentState: AgentState; prompt: P; params: T }) => StepGenerator\n"},{"path":"common/src/types/dynamic-agent-template.ts","preContent":"import { z } from 'zod/v4'\n\nimport { ALLOWED_MODEL_PREFIXES, models } from '../constants'\nimport { toolNames } from '../tools/constants'\n\nimport type { JSONSchema } from 'zod/v4/core'\n\n// Filter models to only include those that begin with allowed prefixes\nconst filteredModels = Object.values(models).filter((model) =>\n  ALLOWED_MODEL_PREFIXES.some((prefix) => model.startsWith(prefix)),\n)\n\nif (filteredModels.length === 0) {\n  throw new Error('No valid models found with allowed prefixes')\n}\n\n// Simplified JSON Schema definition - supports object schemas with nested properties\nconst JsonSchemaSchema: z.ZodType<\n  JSONSchema.BaseSchema,\n  JSONSchema.BaseSchema\n> = z.lazy(() =>\n  z.looseObject({\n    type: z\n      .enum([\n        'object',\n        'array',\n        'string',\n        'number',\n        'boolean',\n        'null',\n        'integer',\n      ])\n      .optional(),\n    description: z.string().optional(),\n    properties: z\n      .record(z.string(), JsonSchemaSchema.or(z.boolean()))\n      .optional(),\n    required: z.string().array().optional(),\n    enum: z\n      .union([z.string(), z.number(), z.boolean(), z.null()])\n      .array()\n      .optional(),\n  }),\n)\nconst JsonObjectSchemaSchema = z.intersection(\n  JsonSchemaSchema,\n  z.object({ type: z.literal('object') }),\n)\n\n// Schema for the combined inputSchema object\nconst InputSchemaObjectSchema = z\n  .looseObject({\n    prompt: z\n      .looseObject({\n        type: z.literal('string'),\n        description: z.string().optional(),\n      })\n      .optional(), // Optional JSON schema for prompt validation\n    params: JsonObjectSchemaSchema.optional(), // Optional JSON schema for params validation\n  })\n  .optional()\n\n// Schema for prompt fields that can be either a string or a path reference\nconst PromptFieldSchema = z.union([\n  z.string(), // Direct string content\n  z.object({ path: z.string() }), // Path reference to external file\n])\nexport type PromptField = z.infer<typeof PromptFieldSchema>\n\nconst functionSchema = <T extends z.core.$ZodFunction>(schema: T) =>\n  z.custom<Parameters<T['implement']>[0]>((fn: any) => schema.implement(fn))\n// Schema for validating handleSteps function signature\nconst HandleStepsSchema = functionSchema(\n  z.function({\n    input: [\n      z.object({\n        agentState: z.object({\n          agentId: z.string(),\n          parentId: z.string(),\n          messageHistory: z.array(z.any()),\n        }),\n        prompt: z.string().optional(),\n        params: z.any().optional(),\n      }),\n    ],\n    output: z.any(),\n  }),\n).optional()\n\n// Validates the Typescript template file.\nexport const DynamicAgentDefinitionSchema = z.object({\n  id: z\n    .string()\n    .regex(\n      /^[a-z0-9-]+$/,\n      'Agent ID must contain only lowercase letters, numbers, and hyphens',\n    ), // The unique identifier for this agent\n  version: z.string().optional(),\n  publisher: z.string().optional(),\n\n  // Required fields for new agents\n  displayName: z.string(),\n  model: z.string(),\n\n  // Tools and spawnable agents\n  toolNames: z.array(z.enum(toolNames)).optional().default([]),\n  spawnableAgents: z.array(z.string()).optional().default([]),\n\n  // Input and output\n  inputSchema: InputSchemaObjectSchema,\n  includeMessageHistory: z.boolean().default(false),\n  outputMode: z\n    .enum(['last_message', 'all_messages', 'structured_output'])\n    .default('last_message'),\n  outputSchema: JsonObjectSchemaSchema.optional(), // Optional JSON schema for output validation\n\n  // Prompts\n  spawnerPrompt: z.string().optional(),\n  systemPrompt: z.string().optional(),\n  instructionsPrompt: z.string().optional(),\n  stepPrompt: z.string().optional(),\n\n  // Optional generator function for programmatic agents\n  handleSteps: z.union([z.string(), HandleStepsSchema]).optional(),\n})\nexport type DynamicAgentDefinition = z.input<\n  typeof DynamicAgentDefinitionSchema\n>\nexport type DynamicAgentDefinitionParsed = z.infer<\n  typeof DynamicAgentDefinitionSchema\n>\n\nexport const DynamicAgentTemplateSchema = DynamicAgentDefinitionSchema.extend({\n  systemPrompt: z.string(),\n  instructionsPrompt: z.string(),\n  stepPrompt: z.string(),\n  handleSteps: z.string().optional(), // Converted to string after processing\n})\n  .refine(\n    (data) => {\n      // If outputSchema is provided, outputMode must be explicitly set to 'structured_output'\n      if (data.outputSchema && data.outputMode !== 'structured_output') {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"outputSchema requires outputMode to be explicitly set to 'structured_output'.\",\n      path: ['outputMode'],\n    },\n  )\n  .refine(\n    (data) => {\n      // If outputMode is 'structured_output', 'set_output' tool must be included\n      if (\n        data.outputMode === 'structured_output' &&\n        !data.toolNames.includes('set_output')\n      ) {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"outputMode 'structured_output' requires the 'set_output' tool. Add 'set_output' to toolNames.\",\n      path: ['toolNames'],\n    },\n  )\n  .refine(\n    (data) => {\n      // If 'set_output' tool is included, outputMode must be 'structured_output'\n      if (\n        data.toolNames.includes('set_output') &&\n        data.outputMode !== 'structured_output'\n      ) {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"'set_output' tool requires outputMode to be 'structured_output'. Change outputMode to 'structured_output' or remove 'set_output' from toolNames.\",\n      path: ['outputMode'],\n    },\n  )\n  .refine(\n    (data) => {\n      // If spawnableAgents array is non-empty, 'spawn_agents' tool must be included\n      if (\n        data.spawnableAgents.length > 0 &&\n        !data.toolNames.includes('spawn_agents')\n      ) {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"Non-empty spawnableAgents array requires the 'spawn_agents' tool. Add 'spawn_agents' to toolNames or remove spawnableAgents.\",\n      path: ['toolNames'],\n    },\n  )\nexport type DynamicAgentTemplate = z.infer<typeof DynamicAgentTemplateSchema>\n","postContent":"import { z } from 'zod/v4'\n\nimport { ALLOWED_MODEL_PREFIXES, models } from '../constants'\nimport { toolNames } from '../tools/constants'\n\nimport type { JSONSchema } from 'zod/v4/core'\n\n// Filter models to only include those that begin with allowed prefixes\nconst filteredModels = Object.values(models).filter((model) =>\n  ALLOWED_MODEL_PREFIXES.some((prefix) => model.startsWith(prefix)),\n)\n\nif (filteredModels.length === 0) {\n  throw new Error('No valid models found with allowed prefixes')\n}\n\n// Simplified JSON Schema definition - supports object schemas with nested properties\nconst JsonSchemaSchema: z.ZodType<\n  JSONSchema.BaseSchema,\n  JSONSchema.BaseSchema\n> = z.lazy(() =>\n  z.looseObject({\n    type: z\n      .enum([\n        'object',\n        'array',\n        'string',\n        'number',\n        'boolean',\n        'null',\n        'integer',\n      ])\n      .optional(),\n    description: z.string().optional(),\n    properties: z\n      .record(z.string(), JsonSchemaSchema.or(z.boolean()))\n      .optional(),\n    required: z.string().array().optional(),\n    enum: z\n      .union([z.string(), z.number(), z.boolean(), z.null()])\n      .array()\n      .optional(),\n  }),\n)\nconst JsonObjectSchemaSchema = z.intersection(\n  JsonSchemaSchema,\n  z.object({ type: z.literal('object') }),\n)\n\n// Schema for the combined inputSchema object\nconst InputSchemaObjectSchema = z\n  .looseObject({\n    prompt: z\n      .looseObject({\n        type: z.literal('string'),\n        description: z.string().optional(),\n      })\n      .optional(), // Optional JSON schema for prompt validation\n    params: JsonObjectSchemaSchema.optional(), // Optional JSON schema for params validation\n  })\n  .optional()\n\n// Schema for prompt fields that can be either a string or a path reference\nconst PromptFieldSchema = z.union([\n  z.string(), // Direct string content\n  z.object({ path: z.string() }), // Path reference to external file\n])\nexport type PromptField = z.infer<typeof PromptFieldSchema>\n\nconst functionSchema = <T extends z.core.$ZodFunction>(schema: T) =>\n  z.custom<Parameters<T['implement']>[0]>((fn: any) => schema.implement(fn))\n// Schema for validating handleSteps function signature\nconst HandleStepsSchema = functionSchema(\n  z.function({\n    input: [\n      z.object({\n        agentState: z.object({\n          agentId: z.string(),\n          parentId: z.string(),\n          messageHistory: z.array(z.any()),\n        }),\n        prompt: z.string().optional(),\n        params: z.any().optional(),\n      }),\n    ],\n    output: z.any(),\n  }),\n).optional()\n\n// Validates the Typescript template file.\nexport const DynamicAgentDefinitionSchema = z.object({\n  id: z\n    .string()\n    .regex(\n      /^[a-z0-9-]+$/,\n      'Agent ID must contain only lowercase letters, numbers, and hyphens',\n    ), // The unique identifier for this agent\n  version: z.string().optional(),\n  publisher: z.string().optional(),\n\n  // Required fields for new agents\n  displayName: z.string(),\n  model: z.string(),\n  reasoningOptions: z\n    .object({\n      enabled: z.boolean().optional(),\n      exclude: z.boolean().optional(),\n    })\n    .and(\n      z.union([\n        z.object({ max_tokens: z.number() }),\n        z.object({ effort: z.enum(['high', 'medium', 'low']) }),\n      ]),\n    )\n    .optional(),\n\n  // Tools and spawnable agents\n  toolNames: z.array(z.enum(toolNames)).optional().default([]),\n  spawnableAgents: z.array(z.string()).optional().default([]),\n\n  // Input and output\n  inputSchema: InputSchemaObjectSchema,\n  includeMessageHistory: z.boolean().default(false),\n  outputMode: z\n    .enum(['last_message', 'all_messages', 'structured_output'])\n    .default('last_message'),\n  outputSchema: JsonObjectSchemaSchema.optional(), // Optional JSON schema for output validation\n\n  // Prompts\n  spawnerPrompt: z.string().optional(),\n  systemPrompt: z.string().optional(),\n  instructionsPrompt: z.string().optional(),\n  stepPrompt: z.string().optional(),\n\n  // Optional generator function for programmatic agents\n  handleSteps: z.union([z.string(), HandleStepsSchema]).optional(),\n})\nexport type DynamicAgentDefinition = z.input<\n  typeof DynamicAgentDefinitionSchema\n>\nexport type DynamicAgentDefinitionParsed = z.infer<\n  typeof DynamicAgentDefinitionSchema\n>\n\nexport const DynamicAgentTemplateSchema = DynamicAgentDefinitionSchema.extend({\n  systemPrompt: z.string(),\n  instructionsPrompt: z.string(),\n  stepPrompt: z.string(),\n  handleSteps: z.string().optional(), // Converted to string after processing\n})\n  .refine(\n    (data) => {\n      // If outputSchema is provided, outputMode must be explicitly set to 'structured_output'\n      if (data.outputSchema && data.outputMode !== 'structured_output') {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"outputSchema requires outputMode to be explicitly set to 'structured_output'.\",\n      path: ['outputMode'],\n    },\n  )\n  .refine(\n    (data) => {\n      // If outputMode is 'structured_output', 'set_output' tool must be included\n      if (\n        data.outputMode === 'structured_output' &&\n        !data.toolNames.includes('set_output')\n      ) {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"outputMode 'structured_output' requires the 'set_output' tool. Add 'set_output' to toolNames.\",\n      path: ['toolNames'],\n    },\n  )\n  .refine(\n    (data) => {\n      // If 'set_output' tool is included, outputMode must be 'structured_output'\n      if (\n        data.toolNames.includes('set_output') &&\n        data.outputMode !== 'structured_output'\n      ) {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"'set_output' tool requires outputMode to be 'structured_output'. Change outputMode to 'structured_output' or remove 'set_output' from toolNames.\",\n      path: ['outputMode'],\n    },\n  )\n  .refine(\n    (data) => {\n      // If spawnableAgents array is non-empty, 'spawn_agents' tool must be included\n      if (\n        data.spawnableAgents.length > 0 &&\n        !data.toolNames.includes('spawn_agents')\n      ) {\n        return false\n      }\n      return true\n    },\n    {\n      message:\n        \"Non-empty spawnableAgents array requires the 'spawn_agents' tool. Add 'spawn_agents' to toolNames or remove spawnableAgents.\",\n      path: ['toolNames'],\n    },\n  )\nexport type DynamicAgentTemplate = z.infer<typeof DynamicAgentTemplateSchema>\n"}]}